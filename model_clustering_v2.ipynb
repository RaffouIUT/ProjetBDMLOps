{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\leono\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre optimal de clusters selon l'analyse de silhouette est : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/10 17:22:36 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod√®le K-Means enregistr√© avec MLflow !\n",
      "üèÉ View run traveling-shark-678 at: http://localhost:5000/#/experiments/1/runs/d021b8f7fe1a4ab6816e740c32cd6095\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n",
      "Th√®mes des clusters :\n",
      "Cluster 1: build\n",
      "Cluster 0: ii\n",
      "Mise √† jour MongoDB termin√©e !\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Connexion √† MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27018\")\n",
    "db = client[\"my_database\"]\n",
    "collection = db[\"my_collection\"]\n",
    "\n",
    "# Charger les donn√©es\n",
    "data = list(collection.find({}, {\"_id\": 1, \"Nom du d√©p√¥t\": 1, \"Topics\": 1, \"Description\": 1, \"README\": 1}))\n",
    "\n",
    "# Convertir en DataFrame Pandas\n",
    "df = pd.DataFrame(data)\n",
    "df[\"combined_text\"] = df[\"Nom du d√©p√¥t\"] + \" \" + df[\"Topics\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else \"\") + \" \" + df[\"Description\"] + \" \" + df[\"README\"]\n",
    "\n",
    "# T√©l√©charger les stopwords de nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Fonction de nettoyage\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convertir en minuscules\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Supprimer la ponctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Supprimer les chiffres\n",
    "    tokens = text.split()  # Tokeniser\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Supprimer les stopwords\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Appliquer le nettoyage\n",
    "df[\"cleaned_text\"] = df[\"combined_text\"].apply(clean_text)\n",
    "\n",
    "# Charger un mod√®le de Sentence Transformers - embeddings plus riches\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Convertir les textes en embeddings\n",
    "df[\"embedding\"] = list(model.encode(df[\"cleaned_text\"].tolist()))\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "# Initialiser MLflow\n",
    "mlflow.set_experiment(\"KMeans Clustering Experiment\")\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # √âviter les erreurs de leak de m√©moire\n",
    "\n",
    "# D√©terminer le nombre optimal de clusters avec la silhouette analysis\n",
    "silhouette_scores = []\n",
    "inertia = []\n",
    "K = range(2, min(10, df.shape[0] + 1))  # Limiter K au nombre d'√©chantillons\n",
    "\n",
    "with mlflow.start_run() as run:  # D√©marrer une nouvelle run MLflow\n",
    "    run_id = run.info.run_id  # R√©cup√©rer l'ID de la run\n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(df[\"embedding\"].tolist())\n",
    "        score = silhouette_score(df[\"embedding\"].tolist(), labels)\n",
    "        \n",
    "        silhouette_scores.append(score)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "\n",
    "        # Enregistrer les m√©triques dans MLflow\n",
    "        mlflow.log_metric(f\"silhouette_score_k{k}\", score)\n",
    "        mlflow.log_metric(f\"inertia_k{k}\", kmeans.inertia_)\n",
    "\n",
    "    # Tracer la courbe d'inertie et l'enregistrer\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(K, inertia, marker='o')\n",
    "    plt.xlabel('Nombre de clusters (k)')\n",
    "    plt.ylabel('Inertie')\n",
    "    plt.title('M√©thode du coude pour choisir k')\n",
    "    plt.savefig(\"mlruns/elbow_method.png\")\n",
    "    mlflow.log_artifact(\"mlruns/elbow_method.png\")  # Enregistrer l‚Äôimage dans MLflow\n",
    "    plt.close()\n",
    "\n",
    "    # Tracer le score de silhouette et l'enregistrer\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(K, silhouette_scores, marker='o')\n",
    "    plt.xlabel('Nombre de clusters (k)')\n",
    "    plt.ylabel('Score de silhouette')\n",
    "    plt.title('Analyse de silhouette pour choisir k')\n",
    "    plt.savefig(\"mlruns/silhouette_analysis.png\")\n",
    "    mlflow.log_artifact(\"mlruns/silhouette_analysis.png\")  # Enregistrer l‚Äôimage dans MLflow\n",
    "    plt.close()\n",
    "\n",
    "    # Trouver le meilleur score de silhouette\n",
    "    best_silhouette_score = max(silhouette_scores)\n",
    "\n",
    "    # Trouver le k optimal\n",
    "    optimal_k = K[silhouette_scores.index(best_silhouette_score)]\n",
    "    mlflow.log_param(\"optimal_k\", optimal_k)  # Enregistrer k optimal\n",
    "\n",
    "    # Sauvegarde du score de silhouette pour k optimal\n",
    "    silhouette_collection = db[\"silhouette_scores\"]  # Nouvelle collection pour stocker les scores\n",
    "    silhouette_data = {\n",
    "            \"best_silhouette_score\": float(best_silhouette_score),\n",
    "            \"run_id\": run_id,\n",
    "            \"optimal_k\": optimal_k\n",
    "        }\n",
    "\n",
    "    silhouette_collection.insert_one(silhouette_data)\n",
    "\n",
    "\n",
    "    print(f\"Le nombre optimal de clusters selon l'analyse de silhouette est : {optimal_k}\")\n",
    "\n",
    "    # Appliquer K-Means avec le k optimal\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "    df[\"cluster\"] = kmeans.fit_predict(df[\"embedding\"].tolist())\n",
    "\n",
    "    # Enregistrer le mod√®le K-Means dans MLflow\n",
    "    mlflow.sklearn.log_model(kmeans, \"KMeans_Model\")\n",
    "\n",
    "    print(\"Mod√®le K-Means enregistr√© avec MLflow !\")\n",
    "\n",
    "    # Regrouper les textes par cluster\n",
    "clusters = df[\"cluster\"].unique()\n",
    "cluster_texts = {cluster: df[df[\"cluster\"] == cluster][\"cleaned_text\"].tolist() for cluster in clusters}\n",
    "cluster_themes = {}\n",
    "cluster_centroids = {cluster: np.mean(df[df[\"cluster\"] == cluster][\"embedding\"].tolist(), axis=0) for cluster in clusters}\n",
    "\n",
    "for cluster, texts in cluster_texts.items():\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7, min_df=2, ngram_range=(1, 1))\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    tfidf_scores = np.mean(tfidf_matrix.toarray(), axis=0)\n",
    "    top_indices = np.argsort(tfidf_scores)[::-1][:10]\n",
    "    top_keywords = [feature_names[i] for i in top_indices]\n",
    "    \n",
    "    if not top_keywords:\n",
    "        cluster_themes[cluster] = \"Unknown\"\n",
    "        continue\n",
    "    \n",
    "    keyword_embeddings = model.encode(top_keywords)\n",
    "    centroid = cluster_centroids[cluster]\n",
    "    similarities = np.dot(keyword_embeddings, centroid)\n",
    "    \n",
    "    best_keyword_index = np.argmax(similarities)\n",
    "    best_keyword = top_keywords[best_keyword_index]\n",
    "    \n",
    "    other_centroids = [cluster_centroids[c] for c in clusters if c != cluster]\n",
    "    other_similarities = [np.dot(model.encode(best_keyword), c) for c in other_centroids]\n",
    "    \n",
    "    if max(other_similarities) > 0.1:\n",
    "        similarities[best_keyword_index] = -1\n",
    "        best_keyword_index = np.argmax(similarities)\n",
    "        best_keyword = top_keywords[best_keyword_index]\n",
    "    \n",
    "    cluster_themes[cluster] = best_keyword\n",
    "\n",
    "print(\"Th√®mes des clusters :\")\n",
    "for cluster, theme in cluster_themes.items():\n",
    "    print(f\"Cluster {cluster}: {theme}\")\n",
    "\n",
    "# Mettre √† jour chaque document avec l'embedding et le th√®me\n",
    "for _, row in df.iterrows():\n",
    "    query = {\"_id\": row[\"_id\"]}  # Assure-toi que cette cl√© est unique\n",
    "\n",
    "    update_data = {\n",
    "        \"$set\": {\n",
    "            \"embedding\": row[\"embedding\"].tolist(),  # Convertir en liste pour MongoDB\n",
    "            \"theme\": cluster_themes[row[\"cluster\"]]  # Associer le th√®me d√©tect√©\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    collection.update_one(query, update_data, upsert=True)  # Met √† jour ou ins√®re si inexistant\n",
    "\n",
    "print(\"Mise √† jour MongoDB termin√©e !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Nouvelle donn√©e class√©e sous le th√®me : build\n",
      "‚úÖ Nouvelle donn√©e ins√©r√©e dans MongoDB !\n",
      "üìä Ancien score silhouette : 0.14329692721366882\n",
      "üìå k_optimal stock√© en base : 2\n",
      "üìä Nouveau score silhouette : 0.14290698002625035\n",
      "üîπ Pas de recalcul n√©cessaire, conservation des clusters existants.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "\n",
    "\n",
    "# Charger un mod√®le de Sentence Transformers - embeddings plus riches\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Fonction de nettoyage\n",
    "def clean_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()  # Convertir en minuscules\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Supprimer la ponctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Supprimer les chiffres\n",
    "    tokens = text.split()  # Tokeniser\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Supprimer les stopwords\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def get_cluster_info_from_mongo(collection):\n",
    "    \"\"\"\n",
    "    R√©cup√®re les embeddings et les th√®mes depuis MongoDB, regroupe par th√®me et calcule les centro√Ødes.\n",
    "\n",
    "    Param√®tres :\n",
    "    - collection : Collection MongoDB contenant les donn√©es.\n",
    "\n",
    "    Retourne :\n",
    "    - cluster_centroids (dict) : Centro√Ødes des clusters sous forme de dictionnaire {th√®me: centroid_embedding}.\n",
    "    - cluster_themes (dict) : Dictionnaire associant chaque cluster √† un th√®me {cluster_id: th√®me}.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1Ô∏è‚É£ R√©cup√©ration des donn√©es depuis MongoDB\n",
    "    data = list(collection.find({}, {\"_id\": 0, \"theme\": 1, \"embedding\": 1}))\n",
    "\n",
    "    # 2Ô∏è‚É£ V√©rification des donn√©es\n",
    "    if not data:\n",
    "        raise ValueError(\"Aucune donn√©e trouv√©e dans la base MongoDB ! üö®\")\n",
    "\n",
    "    # 3Ô∏è‚É£ Regrouper les embeddings par th√®me\n",
    "    theme_embeddings = {}\n",
    "    for entry in data:\n",
    "        theme = entry[\"theme\"]\n",
    "        embedding = entry[\"embedding\"]\n",
    "\n",
    "        if theme not in theme_embeddings:\n",
    "            theme_embeddings[theme] = []\n",
    "        theme_embeddings[theme].append(embedding)\n",
    "\n",
    "    # 4Ô∏è‚É£ Calcul des centro√Ødes des clusters\n",
    "    cluster_centroids = {theme: np.mean(embeddings, axis=0) for theme, embeddings in theme_embeddings.items()}\n",
    "\n",
    "    # 5Ô∏è‚É£ Associer un ID num√©rique √† chaque th√®me pour le mapping\n",
    "    cluster_themes = {idx: theme for idx, theme in enumerate(theme_embeddings.keys())}\n",
    "\n",
    "    return cluster_centroids, cluster_themes\n",
    "\n",
    "def classify_new_data(new_data, model, cluster_centroids, cluster_themes):\n",
    "    \"\"\"\n",
    "    Classifie une nouvelle donn√©e en utilisant les centro√Ødes des clusters r√©cup√©r√©s depuis MongoDB.\n",
    "\n",
    "    Param√®tres :\n",
    "    - new_data (dict) : Contient \"Nom du d√©p√¥t\", \"Topics\", \"Description\", \"README\".\n",
    "    - model (SentenceTransformer) : Mod√®le SentenceTransformer pour g√©n√©rer l'embedding.\n",
    "    - cluster_centroids (dict) : Centro√Ødes des clusters {th√®me: centroid_embedding}.\n",
    "    - cluster_themes (dict) : Association entre ID de cluster et th√®me {cluster_id: th√®me}.\n",
    "\n",
    "    Retourne :\n",
    "    - theme (str) : Th√®me pr√©dit pour la nouvelle donn√©e.\n",
    "    - embedding (list) : L'embedding de la nouvelle donn√©e.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1Ô∏è‚É£ Concat√©ner et nettoyer le texte\n",
    "    combined_text = (\n",
    "        new_data[\"Nom du d√©p√¥t\"] + \" \" +\n",
    "        (\" \".join(new_data[\"Topics\"]) if isinstance(new_data[\"Topics\"], list) else \"\") + \" \" +\n",
    "        new_data[\"Description\"] + \" \" +\n",
    "        new_data[\"README\"]\n",
    "    )\n",
    "    cleaned_text = clean_text(combined_text)\n",
    "\n",
    "    # 2Ô∏è‚É£ G√©n√©rer l'embedding\n",
    "    embedding = model.encode([cleaned_text])[0]\n",
    "\n",
    "    # 3Ô∏è‚É£ Comparer avec les centro√Ødes des clusters\n",
    "    themes = list(cluster_centroids.keys())  # Liste des th√®mes\n",
    "    centroids = np.array([cluster_centroids[theme] for theme in themes])\n",
    "\n",
    "    similarities = cosine_similarity([embedding], centroids)[0]\n",
    "    best_theme = themes[np.argmax(similarities)]  # Trouver le th√®me avec la meilleure similarit√©\n",
    "\n",
    "    return best_theme, embedding.tolist()\n",
    "\n",
    "\n",
    "# Fonction de traitement et de mise √† jour de MongoDB\n",
    "def kmeans_clustering_and_update_mongodb():\n",
    "    # Connexion √† MongoDB\n",
    "    client = MongoClient(\"mongodb://localhost:27018\")\n",
    "    db = client[\"my_database\"]\n",
    "    collection = db[\"my_collection\"]\n",
    "\n",
    "    # Charger les donn√©es\n",
    "    data = list(collection.find({}, {\"_id\": 1, \"Nom du d√©p√¥t\": 1, \"Topics\": 1, \"Description\": 1, \"README\": 1, \"embedding\": 1}))\n",
    "    \n",
    "    # Convertir en DataFrame Pandas\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Combiner les textes\n",
    "    df[\"combined_text\"] = df[\"Nom du d√©p√¥t\"] + \" \" + df[\"Topics\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else \"\") + \" \" + df[\"Description\"] + \" \" + df[\"README\"]\n",
    "\n",
    "    # T√©l√©charger les stopwords de nltk\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    # Appliquer le nettoyage\n",
    "    df[\"cleaned_text\"] = df[\"combined_text\"].apply(clean_text)\n",
    "\n",
    "    # Utiliser les embeddings existants (pas de recalcul)\n",
    "    # Assurez-vous que les embeddings sont sous forme de listes\n",
    "    df[\"embedding\"] = df[\"embedding\"].apply(lambda x: json.loads(x) if isinstance(x, str) else x)  # Assurez-vous que les embeddings sont en liste\n",
    "\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "    # Initialiser MLflow\n",
    "    mlflow.set_experiment(\"KMeans Clustering Experiment\")\n",
    "\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # √âviter les erreurs de leak de m√©moire\n",
    "\n",
    "    # D√©terminer le nombre optimal de clusters avec l'analyse de silhouette\n",
    "    silhouette_scores = []\n",
    "    inertia = []\n",
    "    K = range(2, min(10, df.shape[0] + 1))  # Limiter K au nombre d'√©chantillons\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        for k in K:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            labels = kmeans.fit_predict(df[\"embedding\"].tolist())\n",
    "            score = silhouette_score(df[\"embedding\"].tolist(), labels)\n",
    "            \n",
    "            silhouette_scores.append(score)\n",
    "            inertia.append(kmeans.inertia_)\n",
    "\n",
    "            # Enregistrer les m√©triques dans MLflow\n",
    "            mlflow.log_metric(f\"silhouette_score_k{k}\", score)\n",
    "            mlflow.log_metric(f\"inertia_k{k}\", kmeans.inertia_)\n",
    "\n",
    "        # Tracer la courbe d'inertie et l'enregistrer\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(K, inertia, marker='o')\n",
    "        plt.xlabel('Nombre de clusters (k)')\n",
    "        plt.ylabel('Inertie')\n",
    "        plt.title('M√©thode du coude pour choisir k')\n",
    "        plt.savefig(\"mlruns/elbow_method.png\")\n",
    "        mlflow.log_artifact(\"mlruns/elbow_method.png\")  # Enregistrer l‚Äôimage dans MLflow\n",
    "        plt.close()\n",
    "\n",
    "        # Tracer le score de silhouette et l'enregistrer\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(K, silhouette_scores, marker='o')\n",
    "        plt.xlabel('Nombre de clusters (k)')\n",
    "        plt.ylabel('Score de silhouette')\n",
    "        plt.title('Analyse de silhouette pour choisir k')\n",
    "        plt.savefig(\"mlruns/silhouette_analysis.png\")\n",
    "        mlflow.log_artifact(\"mlruns/silhouette_analysis.png\")  # Enregistrer l‚Äôimage dans MLflow\n",
    "        plt.close()\n",
    "\n",
    "        # Trouver le k optimal\n",
    "        optimal_k = K[silhouette_scores.index(max(silhouette_scores))]\n",
    "        mlflow.log_param(\"optimal_k\", optimal_k)  # Enregistrer k optimal\n",
    "\n",
    "        print(f\"Le nombre optimal de clusters selon l'analyse de silhouette est : {optimal_k}\")\n",
    "\n",
    "        # Appliquer K-Means avec le k optimal\n",
    "        kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "        df[\"cluster\"] = kmeans.fit_predict(df[\"embedding\"].tolist())\n",
    "\n",
    "        # Enregistrer le mod√®le K-Means dans MLflow\n",
    "        mlflow.sklearn.log_model(kmeans, \"KMeans_Model\")\n",
    "\n",
    "        print(\"Mod√®le K-Means enregistr√© avec MLflow !\")\n",
    "\n",
    "        # Regrouper les textes par cluster\n",
    "    clusters = df[\"cluster\"].unique()\n",
    "    cluster_texts = {cluster: df[df[\"cluster\"] == cluster][\"cleaned_text\"].tolist() for cluster in clusters}\n",
    "    cluster_themes = {}\n",
    "    cluster_centroids = {cluster: np.mean(df[df[\"cluster\"] == cluster][\"embedding\"].tolist(), axis=0) for cluster in clusters}\n",
    "\n",
    "    for cluster, texts in cluster_texts.items():\n",
    "        vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7, min_df=2, ngram_range=(1, 1))\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        tfidf_scores = np.mean(tfidf_matrix.toarray(), axis=0)\n",
    "        top_indices = np.argsort(tfidf_scores)[::-1][:10]\n",
    "        top_keywords = [feature_names[i] for i in top_indices]\n",
    "        \n",
    "        if not top_keywords:\n",
    "            cluster_themes[cluster] = \"Unknown\"\n",
    "            continue\n",
    "        \n",
    "        keyword_embeddings = model.encode(top_keywords)\n",
    "        centroid = cluster_centroids[cluster]\n",
    "        similarities = np.dot(keyword_embeddings, centroid)\n",
    "        \n",
    "        best_keyword_index = np.argmax(similarities)\n",
    "        best_keyword = top_keywords[best_keyword_index]\n",
    "        \n",
    "        other_centroids = [cluster_centroids[c] for c in clusters if c != cluster]\n",
    "        other_similarities = [np.dot(model.encode(best_keyword), c) for c in other_centroids]\n",
    "        \n",
    "        if max(other_similarities) > 0.1:\n",
    "            similarities[best_keyword_index] = -1\n",
    "            best_keyword_index = np.argmax(similarities)\n",
    "            best_keyword = top_keywords[best_keyword_index]\n",
    "        \n",
    "        cluster_themes[cluster] = best_keyword\n",
    "\n",
    "    print(\"Th√®mes des clusters :\")\n",
    "    for cluster, theme in cluster_themes.items():\n",
    "        print(f\"Cluster {cluster}: {theme}\")\n",
    "\n",
    "    # Mettre √† jour chaque document avec l'embedding et le th√®me\n",
    "    for _, row in df.iterrows():\n",
    "        query = {\"_id\": row[\"_id\"]}  # Assure-toi que cette cl√© est unique\n",
    "\n",
    "        update_data = {\n",
    "            \"$set\": {\n",
    "                \"embedding\": row[\"embedding\"],  # Conserver l'embedding d√©j√† calcul√©\n",
    "                \"theme\": cluster_themes[row[\"cluster\"]]  # Associer le th√®me d√©tect√©\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        result = collection.update_one(query, update_data, upsert=True)  # Met √† jour ou ins√®re si inexistant\n",
    "        #print(f\"Mise √† jour du d√©p√¥t {row['Nom du d√©p√¥t']}: Matched {result.matched_count}, Modified {result.modified_count}\")\n",
    "\n",
    "    print(\"Mise √† jour MongoDB termin√©e !\")\n",
    "\n",
    "\n",
    "\n",
    "def run(new_repo):\n",
    "    \"\"\"\n",
    "    1Ô∏è‚É£ Classifie une nouvelle donn√©e avec les clusters existants.\n",
    "    2Ô∏è‚É£ R√©cup√®re l'ancien k_optimal et silhouette_score depuis MongoDB.\n",
    "    3Ô∏è‚É£ Recalcule le score de silhouette avec la nouvelle donn√©e.\n",
    "    4Ô∏è‚É£ Compare les scores et met √† jour si n√©cessaire.\n",
    "    \"\"\"\n",
    "    \n",
    "    # üîπ Connexion MongoDB\n",
    "    client = MongoClient(\"mongodb://localhost:27018\")\n",
    "    db = client[\"my_database\"]\n",
    "    collection = db[\"my_collection\"]\n",
    "    silhouette_collection = db[\"silhouette_scores\"]\n",
    "\n",
    "    # üîπ R√©cup√©rer les centro√Ødes et les th√®mes des clusters\n",
    "    cluster_centroids, cluster_themes = get_cluster_info_from_mongo(collection)\n",
    "\n",
    "    # üîπ Classifier la nouvelle donn√©e\n",
    "    predicted_theme, new_embedding = classify_new_data(new_repo, model, cluster_centroids, cluster_themes)\n",
    "\n",
    "    print(f\"üîπ Nouvelle donn√©e class√©e sous le th√®me : {predicted_theme}\")\n",
    "\n",
    "    # üîπ Ins√©rer la nouvelle donn√©e dans MongoDB\n",
    "    collection.insert_one({\n",
    "        **new_repo,\n",
    "        \"embedding\": new_embedding,\n",
    "        \"theme\": predicted_theme\n",
    "    })\n",
    "    print(\"‚úÖ Nouvelle donn√©e ins√©r√©e dans MongoDB !\")\n",
    "\n",
    "    # üîπ R√©cup√©rer l'ancien k_optimal et l'ancien score silhouette depuis MongoDB\n",
    "    silhouette_data = silhouette_collection.find_one({}, {\"_id\": 0, \"best_silhouette_score\": 1, \"optimal_k\": 1})\n",
    "    \n",
    "    if not silhouette_data:\n",
    "        print(\"‚ö†Ô∏è Aucun score silhouette trouv√©, recalcul n√©cessaire !\")\n",
    "        kmeans_clustering_and_update_mongodb()\n",
    "        return\n",
    "    \n",
    "    old_silhouette_score = silhouette_data[\"best_silhouette_score\"]\n",
    "    optimal_k = silhouette_data[\"optimal_k\"]\n",
    "\n",
    "    print(f\"üìä Ancien score silhouette : {old_silhouette_score}\")\n",
    "    print(f\"üìå k_optimal stock√© en base : {optimal_k}\")\n",
    "\n",
    "    # üîπ Recalculer le score silhouette avec la nouvelle donn√©e\n",
    "    df = pd.DataFrame(list(collection.find({}, {\"_id\": 0, \"embedding\": 1})))\n",
    "\n",
    "    if df.shape[0] < optimal_k:\n",
    "        print(\"‚ö†Ô∏è Pas assez de donn√©es pour recalculer le score silhouette. Recalcul des clusters n√©cessaire.\")\n",
    "        kmeans_clustering_and_update_mongodb()\n",
    "        return\n",
    "    \n",
    "    # Convertir les embeddings en liste de vecteurs\n",
    "    df[\"embedding\"] = df[\"embedding\"].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "    embeddings = df[\"embedding\"].tolist()\n",
    "\n",
    "    # Appliquer K-Means avec k_optimal\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    new_silhouette_score = silhouette_score(embeddings, labels)\n",
    "\n",
    "    print(f\"üìä Nouveau score silhouette : {new_silhouette_score}\")\n",
    "\n",
    "    # üîπ Comparer et d√©cider de mettre √† jour ou non\n",
    "    if new_silhouette_score > old_silhouette_score*1.15:\n",
    "        print(\"‚úÖ Nouveau score meilleur, recalcul des clusters !\")\n",
    "        kmeans_clustering_and_update_mongodb()\n",
    "    else:\n",
    "        print(\"üîπ Pas de recalcul n√©cessaire, conservation des clusters existants.\")\n",
    "\n",
    "# Nouvelle donn√©e √† classer\n",
    "new_repo = {\n",
    "    \"Nom du d√©p√¥t\": \"New Repository\",\n",
    "    \"Topics\": [\"Machine Learning\", \"Deep Learning\"],\n",
    "    \"Description\": \"This is a new repository for testing.\",\n",
    "    \"README\": \"This is a test README file.\"\n",
    "}\n",
    "\n",
    "run(new_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Envoi de donn√©es :  {'Nom du d√©p√¥t': 'Exemple de projet', 'Description': 'Projet de test avec des caract√®res sp√©ciaux : √©, √†, √ß', 'Timestamp': datetime.datetime(2025, 3, 10, 20, 43, 41, 61128)}\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration du producteur Kafka\n",
    "conf = {\n",
    "    'bootstrap.servers': 'kafka:9092',\n",
    "    'group.id': 'my-consumer-group',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "# Initialisation du producteur Kafka\n",
    "producer = Producer(conf)\n",
    "\n",
    "# S√©rialiseur personnalis√©\n",
    "def json_serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()  # Convertit datetime en cha√Æne ISO\n",
    "    elif isinstance(obj, set):\n",
    "        return list(obj)  # Convertit les sets en listes\n",
    "    elif isinstance(obj, bytes):\n",
    "        return obj.decode('utf-8')  # Si l'objet est un bytes, on le d√©code en UTF-8\n",
    "    else:\n",
    "        raise TypeError(f\"Type non s√©rialisable: {type(obj)}\")  # Message plus informatif\n",
    "\n",
    "# Fonction de rapport de livraison\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f\"Message failed delivery: {err}\")\n",
    "    else:\n",
    "        print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\n",
    "\n",
    "# Fonction d'envoi de message\n",
    "def send_message(data):\n",
    "    print(\"Envoi de donn√©es : \", data)\n",
    "    try:\n",
    "        # S√©rialiser et envoyer les donn√©es avec UTF-8\n",
    "        producer.produce('topic-new-data', json.dumps(data, default=json_serializer).encode('utf-8'), callback=delivery_report)\n",
    "        producer.flush()  # Assurez-vous que le message est bien envoy√© avant de continuer\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur d'envoi du message: {e}\")\n",
    "\n",
    "# Exemple d'utilisation\n",
    "data = {\n",
    "    \"Nom du d√©p√¥t\": \"Exemple de projet\",\n",
    "    \"Description\": \"Projet de test avec des caract√®res sp√©ciaux : √©, √†, √ß\",\n",
    "    \"Timestamp\": datetime.now()  # Exemple avec un objet datetime\n",
    "}\n",
    "\n",
    "send_message(data)\n",
    "# Arr√™ter le producer proprement\n",
    "producer.flush()  # Attendre que tout soit bien envoy√© avant de quitter\n",
    "producer.close()  # Fermer proprement le producer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No message received\n",
      "No message received\n",
      "No message received\n",
      "No message received\n",
      "Messages consomm√©s: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from confluent_kafka import Consumer\n",
    "\n",
    "conf = {\n",
    "    'bootstrap.servers': 'kafka:9092',\n",
    "    'group.id': 'my-consumer-group',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "consumer = Consumer(conf)\n",
    "consumer.subscribe(['topic-new-data'])\n",
    "\n",
    "def consume_messages():\n",
    "\n",
    "    messages = []\n",
    "    try:\n",
    "        while True:\n",
    "            msg = consumer.poll(1.0)\n",
    "            if msg is None:\n",
    "                print(\"No message received\")\n",
    "                continue\n",
    "            if msg.error():\n",
    "                print(\"Consumer error: {}\".format(msg.error()))\n",
    "                continue\n",
    "            print(f\"Received message: {msg.value().decode('utf-8')}\")\n",
    "            #messages.append(msg.value().decode('utf-8'))\n",
    "            #lancement de la fonction de traitement\n",
    "            run(msg.value().decode('utf-8'))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        consumer.close()\n",
    "\n",
    "    print(f\"Messages consomm√©s: {messages}\")  # Afficher les messages consomm√©s\n",
    "    return messages\n",
    "\n",
    "consume_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model.save('models/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
